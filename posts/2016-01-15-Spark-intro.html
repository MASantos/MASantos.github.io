<h1 id="spark-intro">Spark intro</h1>
<p>Tue 19 Jan 2016 17:21:58 EST</p>
<h2 id="overview">Overview</h2>
<p>Introduction to Spark: single-node installation, pyspark, basic variables and objects.</p>
<p>Index:</p>
<ul>
<li>Overview</li>
<li><a href="#basic-setup-run">Basic Setup &amp; Starting</a></li>
<li><a href="#current-installation">Current Installation</a></li>
<li><a href="#basic-spark-concepts">Basic Spark Concepts</a>
<ul>
<li><a href="#examining-types">Examining types</a></li>
<li><a href="#things-we-can-do-on-lines-rdd">Things we can do on &quot;lines&quot; RDD</a>
<ul>
<li><a href="#functions-and-collect">Functions and collect</a></li>
<li><a href="#map-takeflatmap-reducebykey">map(), take(),flatMap(), reduceByKey(): histograms</a></li>
<li><a href="#word-count">Word Count</a></li>
</ul></li>
</ul></li>
<li><a href="#spark-console-log">Spark Console Output</a>
<ul>
<li><a href="#section-current-installation">Section: Current Installation</a></li>
</ul></li>
</ul>
<h2 id="basic-setup-run"><a href="#overview">Basic Setup &amp; Run</a></h2>
<p>Working with iPython</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ot">IPYTHON=</span>1 <span class="kw">./bin/pyspark</span></code></pre></div>
<p>or with iPython Notebook</p>
<pre><code>IPYTHON_OPTS=&quot;notebook&quot; ./bin/pyspark</code></pre>
<h2 id="current-installation"><a href="#overview">Current Installation</a></h2>
<p>Download Spark from <a href="https://spark.apache.org/downloads.html/">Spark website</a>. For testing purposes, download the package type &quot;pre-built for Hadoop 2.6 or later&quot;, or whatever the version you what to download.</p>
<p>I installed Spark 1.6 in a VirtualBox instance of Ubuntu 14.4 within my MacbookPro under OSX 10.6.8. I want to control spark testing through an ipython notebook that I can access directly from my OSX.</p>
<p>To point iptyhon notebook to listen on a particular ip -without opening a browser on the console right away</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">iptyhon</span> notebook --no-broswer --ip=192.168.56.102</code></pre></div>
<p>The notebook will be accessible on port 8888.</p>
<p>In order to do that with spark we issue</p>
<pre><code>IPYTHON_OPTS=&quot;notebook --no-browser --ip=192.168.56.102&quot; ./bin/pyspark</code></pre>
<p>In this case, we run it from within the directory containing the .bin/ folder of Spark's distribution. All files and paths will be relative to this directory.</p>
<p>It's quite convenient testing/learning with iPython Notebook. If we write</p>
<pre><code>lengthDist.reduceByKey()</code></pre>
<p>and wait a sec, the notebook will show a floating window with the definition of &quot;reduceByKey&quot;:</p>
<pre><code>lengthDist.reduceByKey(self, func, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f6a980938c0&gt;)

Merge the values for each key using an associative reduce function.

This will also perform the merging locally on each mapper before
sending results to a reducer, similarly to a &quot;combiner&quot; in MapReduce.

Output will be partitioned with C{numPartitions} partitions, or
the default parallelism level if C{numPartitions} is not specified.
Default partitioner is hash-partition.

&gt;&gt;&gt; from operator import add
&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])
&gt;&gt;&gt; sorted(rdd.reduceByKey(add).collect())
[(&#39;a&#39;, 2), (&#39;b&#39;, 1)]</code></pre>
<h2 id="basic-spark-concepts"><a href="#overview">Basic Spark concepts</a></h2>
<p>The most basic object in Spark is an RDD, i.e., a <em>Resilient Distributed Dataset</em>. This is a distributed collection whereupon we perform <strong>operations that get automatically parallelized</strong>.</p>
<p>We open a new notebook on the notebook server main page and issue the following</p>
<pre><code>In [1]: lines = sc.textFile(&quot;README.md&quot;)
    lines.count()
Out[1]: 95</code></pre>
<p>Key points:</p>
<ul>
<li>sc creates a &quot;Spark Context&quot;.</li>
<li>lines is an RDD. In this case out of a local file (comes with Spark distribution)</li>
<li>lines.count() does that, indeed, but it does by <strong>collecting all data from all nodes in the cluster</strong>!
<ul>
<li>This is irrelevant in this case, as we are using a single-node setup, but it is important when working on a cluster!</li>
</ul></li>
</ul>
<p>Note: If you are using Firefox with RequestPolicy addon, it will make life easier if you just allow (even if only temporarilly) all out and inbound connections from the notebook server address ( 192.168.56.102 in this case). Otherwise, neither shift+enter nor ctrl+enter will work in order to execute the commands entered.</p>
<h3 id="examining-types"><a href="#overview">Examining types</a></h3>
<pre><code>In [3]: sc 
Out[3]: &lt;pyspark.context.SparkContext at 0x7f6a9809fa10&gt;

In [4]: lines 
Out[4]: MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2</code></pre>
<h3 id="things-we-can-do-on-lines-rdd"><a href="#overview">Things we can do on &quot;lines&quot; RDD</a></h3>
<pre><code>pythonLines = lines.filter(lambda line: &quot;python&quot; in line)</code></pre>
<p>If we press (shift+)enter we can see there is no output, not even in the console. Spark has only defined &quot;pythonLines&quot;, but not proceeded with its corresponding calculation yet!</p>
<p>Its type:</p>
<pre><code>In [6]: pythonLines 
Out[6]: PythonRDD[4] at RDD at PythonRDD.scala:43</code></pre>
<p>In this version of Spark, the output RDD (&quot;pythonLines&quot;) is empty.</p>
<pre><code>In [9]: pythonLines.count() 
Out[9]: 0</code></pre>
<h3 id="functions-and-collect"><a href="#overview">Functions and collect</a></h3>
<p>Let's try with searching for &quot;spark&quot;. We can pass filter a function defined by ourselves</p>
<pre><code>In [11]: def hasSpark(line): 
            return &quot;spark&quot; in line;
     sparkLines = lines.filter(hasSpark)
In [12]: sparkLines.count() 
Out[12]: 10</code></pre>
<p>We can see the whole content of spakLines with 'collect()'.</p>
<pre><code>In [13]: sparkLines.collect() 
Out[13]: 
[u&#39;&lt;http://spark.apache.org/&gt;&#39;,
 u&#39;guide, on the [project web page](http://spark.apache.org/documentation.html)&#39;,
 u&#39;[&quot;Building Spark&quot;](http://spark.apache.org/docs/latest/building-spark.html).&#39;,
 u&#39;    ./bin/spark-shell&#39;,
 u&#39;    ./bin/pyspark&#39;,
 u&#39;examples to a cluster. This can be a mesos:// or spark:// URL,&#39;,
 u&#39;    MASTER=spark://host:7077 ./bin/run-example SparkPi&#39;,
 u&#39;Testing first requires [building Spark](#building-spark). Once Spark is built, tests&#39;,
 u&#39;[&quot;Specifying the Hadoop Version&quot;](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)&#39;,
 u&#39;Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)&#39;]</code></pre>
<h3 id="map-takeflatmap-reducebykey"><a href="#overview">map(), take(),flatMap(), reduceByKey()</a></h3>
<p>Let's get a distribution of line lengths.<br />
First we will cull the length of a few lines. We will use a list of pairs build as follows:</p>
<pre><code>In [29]: lengthDist=sparkLines.map(lambda line: (len(line),line)) 
In [31]: lengthDist.take(3) 
Out[31]: 
[(26, u&#39;&lt;http://spark.apache.org/&gt;&#39;),
 (76,
  u&#39;guide, on the [project web page](http://spark.apache.org/documentation.html)&#39;),
 (76,
  u&#39;[&quot;Building Spark&quot;](http://spark.apache.org/docs/latest/building-spark.html).&#39;)]</code></pre>
<p>For the distribution we just need the line length. Thus</p>
<pre><code>In [14]: lengthDist=lines.map(lambda line: (len(line),1)) 
In [15]: lengthDist.take(3) 
Out[15]: [(14, 1), (0, 1), (78, 1)] 
In [17]: dist=lengthDist.reduceByKey(lambda a,b: a+b).sortByKey().collect() 
In [23]: dist[len(dist)-4:] 
Out[23]: [(78, 1), (84, 2), (97, 1), (120, 2)] 
In [24]: %matplotlib inline 
    import matplotlib.pyplot as plt
    plt.plot([a for (a,b) in dist],[b for (a,b) in dist]) 
    plt.title(&quot;Line length distribution in README.md&quot;)
    plt.xlabel(&quot;len&quot;)
    plt.ylabel(&quot;freq&quot;)
Out[24]: [&lt;matplotlib.lines.Line2D at 0x7f4b507d2390&gt;]</code></pre>
<p>The plot we obtain is<br />
<img src="../images/linesDist.png" alt="Line length distribution in README.md" /></p>
<h3 id="word-count"><a href="#overview">Word Count</a></h3>
<p>Let's count the word distribution in README.md</p>
<pre><code>In [26]: wordDist=lines.flatMap(lambda line: line.split()).map(lambda word: (word,1)).reduceByKey(lambda a,b:a+b) 
     wordDist.collect()[:4]
Out[26]: [(u&#39;help&#39;, 1), (u&#39;when&#39;, 1), (u&#39;Hadoop&#39;, 3), (u&#39;&quot;local&quot;&#39;, 1)]</code></pre>
<p>We want to plot a red histogram shows counts from most frequent word to least frequent. Word will be simply indexed.</p>
<pre><code>In [42]: wordDist=lines.flatMap(lambda line: line.split())
.map(lambda word: (word,1))
.reduceByKey(lambda a,b:a+b)
.sortBy(lambda x: x[1],ascending=False)
.collect() 
wordDist[:4] 
Out[42]: [(u&#39;the&#39;, 21), (u&#39;to&#39;, 14), (u&#39;Spark&#39;, 13), (u&#39;for&#39;, 11)]

In [46]: plt.plot([b for (a,b) in wordDist],&#39;r&#39;) 
     plt.title(&quot;Word count in README.md&quot;)
    plt.xlabel(&quot;Distinct word (index)&quot;)
    plt.ylabel(&quot;Freq&quot;)
Out[46]: &lt;matplotlib.text.Text at 0x7f4b502cdd50&gt;</code></pre>
<p>which yields this plot<br />
<img src="../images/wordCount.png" alt="Word count distribution in README.md" /></p>
<p>X-axis corresponds to a different index for each distinct word sort from most frequent to least.</p>
<h2 id="spark-console-log"><a href="#overview">Spark Console Log</a></h2>
<h3 id="section-current-installation"><a href="#overview">Section: Current Installation</a></h3>
<pre><code>Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties
16/01/19 15:03:47 INFO SparkContext: Running Spark version 1.6.0
16/01/19 15:03:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/01/19 15:03:49 WARN Utils: Your hostname, DS resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface eth0)
16/01/19 15:03:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/01/19 15:03:49 INFO SecurityManager: Changing view acls to: msantos
16/01/19 15:03:49 INFO SecurityManager: Changing modify acls to: msantos
16/01/19 15:03:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(msantos); users with modify permissions: Set(msantos)
16/01/19 15:03:51 INFO Utils: Successfully started service &#39;sparkDriver&#39; on port 44138.
16/01/19 15:03:52 INFO Slf4jLogger: Slf4jLogger started
16/01/19 15:03:52 INFO Remoting: Starting remoting
16/01/19 15:03:53 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:33886]
16/01/19 15:03:53 INFO Utils: Successfully started service &#39;sparkDriverActorSystem&#39; on port 33886.
16/01/19 15:03:53 INFO SparkEnv: Registering MapOutputTracker
16/01/19 15:03:53 INFO SparkEnv: Registering BlockManagerMaster
16/01/19 15:03:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-92f54403-708c-40dc-af07-77a80af0108f
16/01/19 15:03:53 INFO MemoryStore: MemoryStore started with capacity 517.4 MB
16/01/19 15:03:53 INFO SparkEnv: Registering OutputCommitCoordinator
16/01/19 15:03:55 INFO Utils: Successfully started service &#39;SparkUI&#39; on port 4040.
16/01/19 15:03:55 INFO SparkUI: Started SparkUI at http://10.0.2.15:4040
16/01/19 15:03:56 INFO Executor: Starting executor ID driver on host localhost
16/01/19 15:03:56 INFO Utils: Successfully started service &#39;org.apache.spark.network.netty.NettyBlockTransferService&#39; on port 59372.
16/01/19 15:03:56 INFO NettyBlockTransferService: Server created on 59372
16/01/19 15:03:56 INFO BlockManagerMaster: Trying to register BlockManager
16/01/19 15:03:56 INFO BlockManagerMasterEndpoint: Registering block manager localhost:59372 with 517.4 MB RAM, BlockManagerId(driver, localhost, 59372)
16/01/19 15:03:56 INFO BlockManagerMaster: Registered BlockManager
2016-01-19 15:05:21.011 [NotebookApp] Using existing kernel: 7fac22e6-24fa-45db-b6a8-00cd7bad24e2
2016-01-19 15:05:21.502 [NotebookApp] Connecting to: tcp://127.0.0.1:54595
2016-01-19 15:05:21.514 [NotebookApp] Connecting to: tcp://127.0.0.1:58869
2016-01-19 15:05:21.529 [NotebookApp] Connecting to: tcp://127.0.0.1:56425
16/01/19 15:05:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 153.6 KB, free 153.6 KB)
16/01/19 15:05:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.9 KB, free 167.5 KB)
16/01/19 15:05:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:59372 (size: 13.9 KB, free: 517.4 MB)
16/01/19 15:05:56 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2
16/01/19 15:05:57 INFO FileInputFormat: Total input paths to process : 1
16/01/19 15:05:57 INFO SparkContext: Starting job: count at &lt;ipython-input-1-921bc6c41085&gt;:2
16/01/19 15:05:58 INFO DAGScheduler: Got job 0 (count at &lt;ipython-input-1-921bc6c41085&gt;:2) with 1 output partitions
16/01/19 15:05:58 INFO DAGScheduler: Final stage: ResultStage 0 (count at &lt;ipython-input-1-921bc6c41085&gt;:2)
16/01/19 15:05:58 INFO DAGScheduler: Parents of final stage: List()
16/01/19 15:05:58 INFO DAGScheduler: Missing parents: List()
16/01/19 15:05:58 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at &lt;ipython-input-1-921bc6c41085&gt;:2), which has no missing parents
16/01/19 15:05:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.6 KB, free 173.1 KB)
16/01/19 15:05:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 176.6 KB)
16/01/19 15:05:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:59372 (size: 3.4 KB, free: 517.4 MB)
16/01/19 15:05:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
16/01/19 15:05:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at count at &lt;ipython-input-1-921bc6c41085&gt;:2)
16/01/19 15:05:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/01/19 15:05:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2156 bytes)
16/01/19 15:05:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/01/19 15:05:58 INFO HadoopRDD: Input split: file:/home/msantos/spark-1.6.0-bin-hadoop2.6/README.md:0+3359
16/01/19 15:05:58 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/01/19 15:05:58 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/01/19 15:05:58 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/01/19 15:05:58 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/01/19 15:05:58 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/01/19 15:05:59 INFO PythonRunner: Times: total = 702, boot = 623, init = 74, finish = 5
16/01/19 15:05:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2124 bytes result sent to driver
16/01/19 15:05:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1295 ms on localhost (1/1)
16/01/19 15:05:59 INFO DAGScheduler: ResultStage 0 (count at &lt;ipython-input-1-921bc6c41085&gt;:2) finished in 1.368 s
16/01/19 15:05:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
16/01/19 15:05:59 INFO DAGScheduler: Job 0 finished: count at &lt;ipython-input-1-921bc6c41085&gt;:2, took 1.921978 s
16/01/19 15:14:52 INFO SparkContext: Starting job: runJob at PythonRDD.scala:393
16/01/19 15:14:52 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:393) with 1 output partitions
16/01/19 15:14:52 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:393)
16/01/19 15:14:52 INFO DAGScheduler: Parents of final stage: List()
16/01/19 15:14:52 INFO DAGScheduler: Missing parents: List()
16/01/19 15:14:52 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at RDD at PythonRDD.scala:43), which has no missing parents
16/01/19 15:14:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 181.3 KB)
16/01/19 15:14:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 184.3 KB)
16/01/19 15:14:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:59372 (size: 3.0 KB, free: 517.4 MB)
16/01/19 15:14:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
16/01/19 15:14:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[3] at RDD at PythonRDD.scala:43)
16/01/19 15:14:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
16/01/19 15:14:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2156 bytes)
16/01/19 15:14:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
16/01/19 15:14:52 INFO HadoopRDD: Input split: file:/home/msantos/spark-1.6.0-bin-hadoop2.6/README.md:0+3359
16/01/19 15:14:52 INFO PythonRunner: Times: total = 61, boot = 37, init = 21, finish = 3
16/01/19 15:14:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2143 bytes result sent to driver
16/01/19 15:14:52 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:393) finished in 0.154 s
16/01/19 15:14:52 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:393, took 0.255668 s
16/01/19 15:14:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 155 ms on localhost (1/1)
16/01/19 15:14:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool</code></pre>
