<h1 id="first-tests-with-hadoop-2.6.5-on-my-macbook-pro-osx-10.6.8">First tests with Hadoop 2.6.5 on my MacBook Pro OSX 10.6.8</h1>
<h2 id="overview">Overview</h2>
<p>Recording what happened while getting a hadoop version running and testing the first java, python and bash scripts as hadoop standalone and streaming runs.</p>
<h2 id="hadoop-install-config">Hadoop install &amp; config</h2>
<p>Installing &amp; running Hadoop in OSX 10.6.8</p>
<ol type="1">
<li><p>Downloaded hadoop 2.7.1 binary. I have JAVA 1.6.0_65. when running the basic first test I get error : &gt;$bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep input test/output 'dfs[a-z.]+' &gt;Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: org/apache/hadoop/util/RunJar : Unsupported major.minor version 51.0 &gt; at java.lang.ClassLoader.defineClass1(Native Method) &gt; at java.lang.ClassLoader.defineClassCond(ClassLoader.java:637) &gt; at java.lang.ClassLoader.defineClass(ClassLoader.java:621) &gt; at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141) &gt; at java.net.URLClassLoader.defineClass(URLClassLoader.java:283) &gt; at java.net.URLClassLoader.access$000(URLClassLoader.java:58) &gt; at java.net.URLClassLoader<span class="math inline">$1.run(URLClassLoader.java:197) &gt; at java.security.AccessController.doPrivileged(Native Method) &gt; at java.net.URLClassLoader.findClass(URLClassLoader.java:190) &gt; at java.lang.ClassLoader.loadClass(ClassLoader.java:306) &gt; at sun.misc.Launcher$</span>AppClassLoader.loadClass(Launcher.java:301) &gt; at java.lang.ClassLoader.loadClass(ClassLoader.java:247) Latest (as of Thu Nov. 12 2015) JDK jdk-8u65-nb-8_1-macosx-x64.dmg requires OSX 8 or higher I downloaded JDK 7_u80, but it requires OS X 10.7.3 or higher</p></li>
<li><p>Downloaded hadoop 2.5.2 binary, installed and ...it worked. At least so far the basic test bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar grep test/input test/output 'dfs[a-z.]+'</p></li>
<li><p>Downloaded hadoop 2.6.2 binary, installed and it worked as well -again, so far tested just the basic test above This is the latest version of the 2.6 branch released Oct 2015. The 2.7.1 version was released on Jul 2015.</p></li>
</ol>
<h2 id="examples">Examples</h2>
<h3 id="data">data</h3>
<p>HCN: NOOA's US historical climate network data.</p>
<h3 id="reference-the-plain-bash-script-no-hadoop">Reference: The plain bash script (no hadoop)</h3>
<p>This will be the reference to compare to simply because it's the <strong>fastest</strong>. Hadoop as standalone can't beat this.</p>
<blockquote>
<p>$time ./max_temp_hcn.sh &gt; hcn-max</p>
<p>real 0m48.068s user 0m35.809s sys 0m8.038s</p>
<p>$sort -n hcn-max | head 1853 348 USC00381310185308TMAX 348 1868 161 USC00144559186810TMAX 161 1869 306 USC00144559186908TMAX 306 1874 256 USW00013724187406TMAX 256 1875 306 USW00013724187507TMAX 306 1876 294 USW00013724187607TMAX 294 1876 306 USW00094728187609TMAX 306 1877 1877 1877 289 USW00013724187709TMAX 289</p>
</blockquote>
<p>Getting now the max per year for all stations: ./max_temp_hcn-all.sh &gt;#!/bin/bash &gt; &gt;cat input/data/NCDC/ghcnd_hcn/*.dly| awk /TMAX/'{ year=substr($0,12,4); temp=substr($0,22,5)+0; &gt; q=substr($0,28,1); &gt; if( temp!=9999 &amp;&amp; q ~ /[[:space:]]/ &amp;&amp; temp &gt; max[year]) { max[year] = temp ; l[year]=substr(<span class="math inline">$0,1,28)} } &gt; END {for( yr in max) print yr,max[yr],l[yr]}' &gt; &gt;$</span>time ./max_temp_hcn-all.sh &gt; hcn-max-all &gt; &gt;real 0m29.214s &gt;user 0m27.698s &gt;sys 0m3.862s</p>
<h2 id="hadoop-standalone-java-mapper-and-reducer">Hadoop standalone: Java Mapper and Reducer</h2>
<blockquote>
<p>$hadoop MaxTemperature input/data/NCDC/ghcnd_hcn/USC00011084.dly output_hcn Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: MaxTemperature Caused by: java.lang.ClassNotFoundException: MaxTemperature at java.net.URLClassLoader<span class="math inline">$1.run(URLClassLoader.java:202)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:190)  at java.lang.ClassLoader.loadClass(ClassLoader.java:306)  at sun.misc.Launcher$</span>AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247)</p>
</blockquote>
<p>Needed to compile: See example HelloWorld &gt;<span class="math inline">$cat HelloWorld.java &gt;public class HelloWorld { &gt; public static void main(String args[]) { &gt; System.out.println(&quot;Hello World!&quot;); &gt; } &gt;} &gt;$</span>java -classpath . HelloWorld.java &gt;<span class="math inline"><em>h</em><em>a</em><em>d</em><em>o</em><em>o</em><em>p</em><em>H</em><em>e</em><em>l</em><em>l</em><em>o</em><em>W</em><em>o</em><em>r</em><em>l</em><em>d</em>.<em>o</em><em>u</em><em>t</em> &gt; <em>H</em><em>e</em><em>l</em><em>l</em><em>o</em><em>W</em><em>o</em><em>r</em><em>l</em><em>d</em>! &gt; <em>m</em><em>s</em><em>a</em><em>n</em><em>t</em><em>o</em><em>s</em>@<em>M</em><em>B</em><em>P</em> − 2[19 : 49] :  /<em>S</em><em>y</em><em>s</em><em>t</em><em>e</em><em>m</em>/<em>H</em><em>A</em><em>D</em><em>O</em><em>O</em><em>P</em>/<em>c</em><em>u</em><em>r</em><em>r</em><em>e</em><em>n</em><em>t</em>/<em>t</em><em>e</em><em>s</em><em>t</em>/<em>c</em><em>l</em><em>a</em><em>s</em><em>s</em><em>e</em><em>s</em></span>javac -classpath <code>hadoop classpath</code> MaxTemperature.java &gt;Note: MaxTemperature.java uses or overrides a deprecated API. &gt;Note: Recompile with -Xlint:deprecation for details. &gt; &gt;msantos@MBP-2[19:53]:~/System/HADOOP/current/test<span class="math inline">$hadoop MaxTemperature input/data/NCDC/ghcnd_hcn/USC00011084.dly output_hcn &gt;15/11/12 19:53:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable &gt;15/11/12 19:53:15 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id &gt;15/11/12 19:53:15 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId= &gt;15/11/12 19:53:16 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this. &gt;15/11/12 19:53:16 WARN mapreduce.JobResourceUploader: No job jar file set. User classes may not be found. See Job or Job#setJar(String). &gt;15/11/12 19:53:16 INFO input.FileInputFormat: Total input paths to process : 1 &gt;15/11/12 19:53:16 INFO mapreduce.JobSubmitter: number of splits:1 &gt;15/11/12 19:53:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1442023825_0001 &gt;15/11/12 19:53:17 INFO mapred.LocalJobRunner: OutputCommitter set in config null &gt;15/11/12 19:53:17 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter &gt;15/11/12 19:53:17 INFO mapreduce.Job: The url to track the job: http://localhost:8080/ &gt;15/11/12 19:53:17 INFO mapreduce.Job: Running job: job_local1442023825_0001 &gt;15/11/12 19:53:17 INFO mapred.LocalJobRunner: Starting task: attempt_local1442023825_0001_m_000000_0 &gt;15/11/12 19:53:17 INFO mapred.LocalJobRunner: Waiting for map tasks &gt;15/11/12 19:53:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux. &gt;15/11/12 19:53:17 INFO mapred.Task: Using ResourceCalculatorProcessTree : null &gt;15/11/12 19:53:17 INFO mapred.MapTask: Processing split: file:/Users/msantos/System/HADOOP/hadoop-2.6.2/test/input/data/NCDC/ghcnd_hcn/USC00011084.dly:0+1700190 &gt;15/11/12 19:53:17 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584) &gt;15/11/12 19:53:17 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100 &gt;15/11/12 19:53:17 INFO mapred.MapTask: soft limit at 83886080 &gt;15/11/12 19:53:17 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600 &gt;15/11/12 19:53:17 INFO mapred.MapTask: kvstart = 26214396; length = 6553600 &gt;15/11/12 19:53:17 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$</span>MapOutputBuffer &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: &gt;15/11/12 19:53:18 INFO mapred.MapTask: Starting flush of map output &gt;15/11/12 19:53:18 INFO mapred.Task: Task:attempt_local1442023825_0001_m_000000_0 is done. And is in the process of committing &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: map &gt;15/11/12 19:53:18 INFO mapred.Task: Task 'attempt_local1442023825_0001_m_000000_0' done. &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local1442023825_0001_m_000000_0 &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: map task executor complete. &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: Waiting for reduce tasks &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: Starting task: attempt_local1442023825_0001_r_000000_0 &gt;15/11/12 19:53:18 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux. &gt;15/11/12 19:53:18 INFO mapred.Task: Using ResourceCalculatorProcessTree : null &gt;15/11/12 19:53:18 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@669a4cb &gt;15/11/12 19:53:18 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=372781856, maxSingleShuffleLimit=93195464, mergeThreshold=246036032, ioSortFactor=10, memToMemMergeOutputsThreshold=10 &gt;15/11/12 19:53:18 INFO reduce.EventFetcher: attempt_local1442023825_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events &gt;15/11/12 19:53:18 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1442023825_0001_m_000000_0 decomp: 2 len: 6 to MEMORY &gt;15/11/12 19:53:18 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1442023825_0001_m_000000_0 &gt;15/11/12 19:53:18 INFO reduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 2, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;2 &gt;15/11/12 19:53:18 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: 1 / 1 copied. &gt;15/11/12 19:53:18 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs &gt;15/11/12 19:53:18 INFO mapred.Merger: Merging 1 sorted segments &gt;15/11/12 19:53:18 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes &gt;15/11/12 19:53:18 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit &gt;15/11/12 19:53:18 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk &gt;15/11/12 19:53:18 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce &gt;15/11/12 19:53:18 INFO mapred.Merger: Merging 1 sorted segments &gt;15/11/12 19:53:18 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: 1 / 1 copied. &gt;15/11/12 19:53:18 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords &gt;15/11/12 19:53:18 INFO mapred.Task: Task:attempt_local1442023825_0001_r_000000_0 is done. And is in the process of committing &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: 1 / 1 copied. &gt;15/11/12 19:53:18 INFO mapred.Task: Task attempt_local1442023825_0001_r_000000_0 is allowed to commit now &gt;15/11/12 19:53:18 INFO mapreduce.Job: Job job_local1442023825_0001 running in uber mode : false &gt;15/11/12 19:53:18 INFO mapreduce.Job: map 100% reduce 0% &gt;15/11/12 19:53:18 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1442023825_0001_r_000000_0' to file:/Users/msantos/System/HADOOP/hadoop-2.6.2/test/output_hcn/_temporary/0/task_local1442023825_0001_r_000000 &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: reduce &gt; reduce &gt;15/11/12 19:53:18 INFO mapred.Task: Task 'attempt_local1442023825_0001_r_000000_0' done. &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local1442023825_0001_r_000000_0 &gt;15/11/12 19:53:18 INFO mapred.LocalJobRunner: reduce task executor complete. &gt;15/11/12 19:53:19 INFO mapreduce.Job: map 100% reduce 100% &gt;15/11/12 19:53:19 INFO mapreduce.Job: Job job_local1442023825_0001 completed successfully &gt;15/11/12 19:53:19 INFO mapreduce.Job: Counters: 30 &gt; File System Counters &gt; FILE: Number of bytes read=3400854 &gt; FILE: Number of bytes written=508728 &gt; FILE: Number of read operations=0 &gt; FILE: Number of large read operations=0 &gt; FILE: Number of write operations=0 &gt; Map-Reduce Framework &gt; Map input records=6297 &gt; Map output records=0 &gt; Map output bytes=0 &gt; Map output materialized bytes=6 &gt; Input split bytes=158 &gt; Combine input records=0 &gt; Combine output records=0 &gt; Reduce input groups=0 &gt; Reduce shuffle bytes=6 &gt; Reduce input records=0 &gt; Reduce output records=0 &gt; Spilled Records=0 &gt; Shuffled Maps =1 &gt; Failed Shuffles=0 &gt; Merged Map outputs=1 &gt; GC time elapsed (ms)=11 &gt; Total committed heap usage (bytes)=395960320 &gt; Shuffle Errors &gt; BAD_ID=0 &gt; CONNECTION=0 &gt; IO_ERROR=0 &gt; WRONG_LENGTH=0 &gt; WRONG_MAP=0 &gt; WRONG_REDUCE=0 &gt; File Input Format Counters &gt; Bytes Read=1700190 &gt; File Output Format Counters &gt; Bytes Written=8 &gt;msantos@MBP-2[19:53]:~/System/HADOOP/current/test$</p>
<p>However there is NO OUTPUT from the reduce task: &quot;Reduce output records=0&quot;. Something is wrong with the java code obvously. What is it really doing? Surely it's not reading the records correctly, i.e., not finding the temperature and TMAX strings. Debugging goal: Lets just read a couple of strings from each record and have the reducer task just print this out. No processing. This changes the type of the map &amp; reduce classes. Their template have 4 types. My guess is first 2 are for their respective input, and the other 2 are for their outputs. However, it turns out this is not enough for hadoop to be able to run it. The java compilation works but hadoop API complains that map is passing an unexpected type: The main interface must declare the right type for the reducer as well!! Here the error message: &gt;15/11/13 12:18:23 WARN mapred.LocalJobRunner: job_local858928967_0001 &gt;java.lang.Exception: java.io.IOException: Type mismatch in value from map: expected org.apache.hadoop.io.IntWritable, received org.apache.hadoop.io.Text &gt; at org.apache.hadoop.mapred.LocalJobRunner<span class="math inline"><em>J</em><em>o</em><em>b</em>.<em>r</em><em>u</em><em>n</em><em>T</em><em>a</em><em>s</em><em>k</em><em>s</em>(<em>L</em><em>o</em><em>c</em><em>a</em><em>l</em><em>J</em><em>o</em><em>b</em><em>R</em><em>u</em><em>n</em><em>n</em><em>e</em><em>r</em>.<em>j</em><em>a</em><em>v</em><em>a</em> : 462) &gt; <em>a</em><em>t</em><em>o</em><em>r</em><em>g</em>.<em>a</em><em>p</em><em>a</em><em>c</em><em>h</em><em>e</em>.<em>h</em><em>a</em><em>d</em><em>o</em><em>o</em><em>p</em>.<em>m</em><em>a</em><em>p</em><em>r</em><em>e</em><em>d</em>.<em>L</em><em>o</em><em>c</em><em>a</em><em>l</em><em>J</em><em>o</em><em>b</em><em>R</em><em>u</em><em>n</em><em>n</em><em>e</em><em>r</em></span>Job.run(LocalJobRunner.java:522) &gt;Caused by: java.io.IOException: Type mismatch in value from map: expected org.apache.hadoop.io.IntWritable, received org.apache.hadoop.io.Text &gt; at org.apache.hadoop.mapred.MapTask<span class="math inline"><em>M</em><em>a</em><em>p</em><em>O</em><em>u</em><em>t</em><em>p</em><em>u</em><em>t</em><em>B</em><em>u</em><em>f</em><em>f</em><em>e</em><em>r</em>.<em>c</em><em>o</em><em>l</em><em>l</em><em>e</em><em>c</em><em>t</em>(<em>M</em><em>a</em><em>p</em><em>T</em><em>a</em><em>s</em><em>k</em>.<em>j</em><em>a</em><em>v</em><em>a</em> : 1074) &gt; <em>a</em><em>t</em><em>o</em><em>r</em><em>g</em>.<em>a</em><em>p</em><em>a</em><em>c</em><em>h</em><em>e</em>.<em>h</em><em>a</em><em>d</em><em>o</em><em>o</em><em>p</em>.<em>m</em><em>a</em><em>p</em><em>r</em><em>e</em><em>d</em>.<em>M</em><em>a</em><em>p</em><em>T</em><em>a</em><em>s</em><em>k</em></span>NewOutputCollector.write(MapTask.java:712) &gt; at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89) &gt; at org.apache.hadoop.mapreduce.lib.map.WrappedMapper<span class="math inline"><em>C</em><em>o</em><em>n</em><em>t</em><em>e</em><em>x</em><em>t</em>.<em>w</em><em>r</em><em>i</em><em>t</em><em>e</em>(<em>W</em><em>r</em><em>a</em><em>p</em><em>p</em><em>e</em><em>d</em><em>M</em><em>a</em><em>p</em><em>p</em><em>e</em><em>r</em>.<em>j</em><em>a</em><em>v</em><em>a</em> : 112) &gt; <em>a</em><em>t</em><em>M</em><em>a</em><em>x</em><em>T</em><em>e</em><em>m</em><em>p</em><em>e</em><em>r</em><em>a</em><em>t</em><em>u</em><em>r</em><em>e</em><em>M</em><em>a</em><em>p</em><em>p</em><em>e</em><em>r</em>.<em>m</em><em>a</em><em>p</em>(<em>M</em><em>a</em><em>x</em><em>T</em><em>e</em><em>m</em><em>p</em><em>e</em><em>r</em><em>a</em><em>t</em><em>u</em><em>r</em><em>e</em><em>M</em><em>a</em><em>p</em><em>p</em><em>e</em><em>r</em>.<em>j</em><em>a</em><em>v</em><em>a</em> : 33)<em>O</em><em>n</em><em>c</em><em>e</em><em>c</em><em>o</em><em>r</em><em>r</em><em>e</em><em>c</em><em>t</em><em>e</em><em>d</em>, <em>c</em><em>o</em><em>m</em><em>p</em><em>i</em><em>l</em><em>e</em><em>d</em><em>a</em><em>n</em><em>d</em><em>r</em><em>u</em><em>n</em><em>w</em><em>e</em><em>g</em><em>e</em><em>t</em> :  &gt; <em>m</em><em>s</em><em>a</em><em>n</em><em>t</em><em>o</em><em>s</em>@<em>M</em><em>B</em><em>P</em> − 2[12 : 21] :  /<em>S</em><em>y</em><em>s</em><em>t</em><em>e</em><em>m</em>/<em>H</em><em>A</em><em>D</em><em>O</em><em>O</em><em>P</em>/<em>c</em><em>u</em><em>r</em><em>r</em><em>e</em><em>n</em><em>t</em>/<em>t</em><em>e</em><em>s</em><em>t</em></span>!hadoop &gt;hadoop MaxTemperature input/data/NCDC/ghcnd_hcn/USC00011084.dly output_hcn &gt;15/11/13 12:21:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable &gt;15/11/13 12:21:46 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id &gt;15/11/13 12:21:46 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId= &gt;15/11/13 12:21:47 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this. &gt;15/11/13 12:21:47 WARN mapreduce.JobResourceUploader: No job jar file set. User classes may not be found. See Job or Job#setJar(String). &gt;15/11/13 12:21:47 INFO input.FileInputFormat: Total input paths to process : 1 &gt;15/11/13 12:21:47 INFO mapreduce.JobSubmitter: number of splits:1 &gt;15/11/13 12:21:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1636203858_0001 &gt;15/11/13 12:21:48 INFO mapred.LocalJobRunner: OutputCommitter set in config null &gt;15/11/13 12:21:48 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter &gt;15/11/13 12:21:48 INFO mapreduce.Job: The url to track the job: http://localhost:8080/ &gt;15/11/13 12:21:48 INFO mapreduce.Job: Running job: job_local1636203858_0001 &gt;15/11/13 12:21:48 INFO mapred.LocalJobRunner: Starting task: attempt_local1636203858_0001_m_000000_0 &gt;15/11/13 12:21:48 INFO mapred.LocalJobRunner: Waiting for map tasks &gt;15/11/13 12:21:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux. &gt;15/11/13 12:21:49 INFO mapred.Task: Using ResourceCalculatorProcessTree : null &gt;15/11/13 12:21:49 INFO mapred.MapTask: Processing split: file:/Users/msantos/System/HADOOP/hadoop-2.6.2/test/input/data/NCDC/ghcnd_hcn/USC00011084.dly:0+1700190 &gt;15/11/13 12:21:49 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584) &gt;15/11/13 12:21:49 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100 &gt;15/11/13 12:21:49 INFO mapred.MapTask: soft limit at 83886080 &gt;15/11/13 12:21:49 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600 &gt;15/11/13 12:21:49 INFO mapred.MapTask: kvstart = 26214396; length = 6553600 &gt;15/11/13 12:21:49 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask<span class="math inline"><em>M</em><em>a</em><em>p</em><em>O</em><em>u</em><em>t</em><em>p</em><em>u</em><em>t</em><em>B</em><em>u</em><em>f</em><em>f</em><em>e</em><em>r</em> &gt; 15/11/1312 : 21 : 49<em>I</em><em>N</em><em>F</em><em>O</em><em>m</em><em>a</em><em>p</em><em>r</em><em>e</em><em>d</em><em>u</em><em>c</em><em>e</em>.<em>J</em><em>o</em><em>b</em> : <em>J</em><em>o</em><em>b</em><em>j</em><em>o</em><em>b</em><sub><em>l</em></sub><em>o</em><em>c</em><em>a</em><em>l</em>1636203858<sub>0001</sub><em>r</em><em>u</em><em>n</em><em>n</em><em>i</em><em>n</em><em>g</em><em>i</em><em>n</em><em>u</em><em>b</em><em>e</em><em>r</em><em>m</em><em>o</em><em>d</em><em>e</em> : <em>f</em><em>a</em><em>l</em><em>s</em><em>e</em> &gt; 15/11/1312 : 21 : 49<em>I</em><em>N</em><em>F</em><em>O</em><em>m</em><em>a</em><em>p</em><em>r</em><em>e</em><em>d</em><em>u</em><em>c</em><em>e</em>.<em>J</em><em>o</em><em>b</em> : <em>m</em><em>a</em><em>p</em>0</span>l output_hcn/ &gt;total 136 &gt;-rw-r--r-- 1 msantos staff 0 13 Nov 12:21 _SUCCESS &gt;-rw-r--r-- 1 msantos staff 69267 13 Nov 12:21 part-r-00000 Now there is an output! &quot;Reduce output records=6297&quot; and so it shows in the output directory where part-r-00000 is no longer empty.</p>
<p>POSTPONED</p>
<h3 id="hadoop-streaming-i-python-mapper-reducer">Hadoop Streaming I : Python Mapper &amp; Reducer</h3>
<p>Let's check the values of TMAX for 1926 as measured by station NCDC/ghcnd_hcn/USC00011084.dly &gt;<span class="math inline">$less ghcnd_hcn/USC00011084.dly|grep &quot;1926.*TMAX&quot; | cut -c 1-26| less &gt;USC00011084192601TMAX-9999 &gt;USC00011084192602TMAX 194 &gt;USC00011084192603TMAX 217 &gt;USC00011084192604TMAX 222 &gt;USC00011084192605TMAX 306 &gt;USC00011084192606TMAX 317 &gt;USC00011084192607TMAX 367 &gt;USC00011084192608TMAX 322 &gt;USC00011084192609TMAX 356 &gt;USC00011084192610TMAX 367 &gt;USC00011084192611TMAX 222 &gt;USC00011084192612TMAX 222 and we see the max temp is 36.7C with python: &gt;$</span>cat ../input/data/NCDC/ghcnd_hcn/USC00011084.dly | ../classes/max_temperature_map.py | ../classes/max_temperature_reduce.py | less &gt;1926 367 &gt;1927 367 &gt;1928 367 &gt;1930 361 &gt;1931 367 &gt;1932 339 &gt;1933 367 &gt;1934 350 &gt;1935 350 &gt;1936 339 &gt;1937 383 &gt;1938 344 &gt;1939 339 &gt;1940 350 &gt;1941 328 &gt;1942 333 &gt;1943 356 &gt;1944 350 &gt;1945 356</p>
<p>now Using hadoop streaming with a pythong script: &gt;OUTDIR=output-py hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-streaming-*.jar<br />
&gt; -input input/data/NCDC/ghcnd_hcn/USC00011084.dly<br />
&gt; -output <span class="math inline">$OUTDIR \
&gt; -mapper classes/max_temperature_map.py \
&gt; -reducer classes/max_temperature_reduce.py \
&gt; &gt;&amp; log-py we get &gt;msantos@MBP-2[15:20]:~/System/HADOOP/current/test$</span>head output-py/part-00000 &gt;1926 367 &gt;1927 367 &gt;1928 367 &gt;1930 361 &gt;1931 367 &gt;1932 339 &gt;1933 367 &gt;1934 350 &gt;1935 350 &gt;1936 339</p>
<p>Python stream on ALL station files (same command as before with input the whole directory input/data/NCDC/ghcnd_hcn): &gt;msantos@MBP-2[15:33]:~/System/HADOOP/current/test$time ./run-MaxTemperature-py &gt; Reduce output records=145 &gt; &gt;real 12m25.303s &gt;user 8m36.415s &gt;sys 4m41.120s</p>
<p>By not checking the quality of the measurement I included a value like this one for 1967 &gt;<span class="math inline">$cat ghcnd_hcn/USC00244364.dly |grep USC00244364196701TMAX &gt;USC00244364196701TMAX 1544 X0-9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 -9999 That is, a max temp of 154.4C !! Obviously this is a failed measurement and the quality indicator correspondingly shows a non-blank character namely 'X'. Filtering for quality we get the final result set. &gt;$</span>time ./run-MaxTemperature-py &gt; Reduce output records=145 &gt; &gt;real 11m44.270s &gt;user 8m34.876s &gt;sys 4m45.707s</p>
<h3 id="hadoop-streaming-ii-python-single-script-usrbincat-as-trivial-reducer">Hadoop Streaming II : Python single script &amp; /usr/bin/cat as trivial reducer</h3>
<p>Streaming single python map reading all files and calculating the max temp for each year &gt;$time ./run-MaxTemperature-py-single &gt; &gt;real 22m22.339s &gt;user 22m2.756s &gt;sys 0m39.069s</p>
<p>Made the mistake of not testing the script first directly on a few sample files. After 22m it turns out it just loaded everything into memory (+600MBi) and crash with error &gt;Traceback (most recent call last): &gt; File &quot;/Users/msantos/System/HADOOP/hadoop-2.6.2/test/./classes/max_temperature_single.py&quot;, line 15, in <module> &gt; if temp&gt;max[year]: &gt;KeyError: '1889'</p>
<h2 id="conclusions">Conclusions</h2>
<h2 id="why-should-i-care">Why Should I Care?</h2>
